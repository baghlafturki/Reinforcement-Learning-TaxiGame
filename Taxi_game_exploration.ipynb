{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Passenger locations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "    - 4: in taxi\n",
    "    \n",
    "    Destinations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "        \n",
    "    Actions:\n",
    "    There are 6 discrete deterministic actions:\n",
    "    - 0: move south\n",
    "    - 1: move north\n",
    "    - 2: move east \n",
    "    - 3: move west \n",
    "    - 4: pickup passenger\n",
    "    - 5: dropoff passenger\n",
    "    \n",
    "    Rewards: \n",
    "    There is a reward of -1 for each action and an additional reward of +20 for delivering the passenger.\n",
    "    There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make('Taxi-v3')\n",
    "Q = defaultdict(lambda: np.array(np.zeros(environment.action_space.n), dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q['s','a']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(eps,state,action_size,Q):\n",
    "    probability = np.array(np.ones(action_size)) * eps/action_size\n",
    "    probability[np.argmax(Q[state])] += 1 - eps\n",
    "    return probability\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, length=1000):\n",
    "        self.memory = deque(maxlen=length)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "        \n",
    "    def sample(self, sample_size = 32):\n",
    "        indexes = np.random.choice(np.arange(len(self.memory)),sample_size,replace=False)\n",
    "        return [self.memory[index] for index in indexes]\n",
    "    \n",
    "    def kickstart(self,env, action_size, batch_size):\n",
    "        state = env.reset()\n",
    "        for _ in range(batch_size):\n",
    "            action = np.random.choice(np.arange(6))\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            print('new state: {} | reward: {}| done: {}|'.format(new_state,reward,done))\n",
    "            self.add((state, action, reward, new_state, done))\n",
    "            state = new_state if not done else env.reset()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new state: 474 | reward: -1| done: False|\n",
      "new state: 474 | reward: -10| done: False|\n",
      "new state: 474 | reward: -1| done: False|\n",
      "new state: 474 | reward: -10| done: False|\n",
      "new state: 478 | reward: -1| done: False|\n",
      "new state: 498 | reward: -1| done: False|\n",
      "new state: 398 | reward: -1| done: False|\n",
      "new state: 398 | reward: -10| done: False|\n",
      "new state: 398 | reward: -1| done: False|\n",
      "new state: 378 | reward: -1| done: False|\n",
      "new state: 378 | reward: -10| done: False|\n",
      "new state: 378 | reward: -1| done: False|\n",
      "new state: 378 | reward: -10| done: False|\n",
      "new state: 478 | reward: -1| done: False|\n",
      "new state: 478 | reward: -1| done: False|\n",
      "new state: 478 | reward: -1| done: False|\n",
      "new state: 378 | reward: -1| done: False|\n",
      "new state: 398 | reward: -1| done: False|\n",
      "new state: 398 | reward: -10| done: False|\n",
      "new state: 378 | reward: -1| done: False|\n",
      "new state: 278 | reward: -1| done: False|\n",
      "new state: 298 | reward: -1| done: False|\n",
      "new state: 298 | reward: -10| done: False|\n",
      "new state: 398 | reward: -1| done: False|\n",
      "new state: 398 | reward: -1| done: False|\n",
      "new state: 298 | reward: -1| done: False|\n",
      "new state: 298 | reward: -1| done: False|\n",
      "new state: 278 | reward: -1| done: False|\n",
      "new state: 298 | reward: -1| done: False|\n",
      "new state: 278 | reward: -1| done: False|\n",
      "new state: 278 | reward: -10| done: False|\n",
      "new state: 178 | reward: -1| done: False|\n",
      "new state: 278 | reward: -1| done: False|\n",
      "new state: 278 | reward: -10| done: False|\n",
      "new state: 298 | reward: -1| done: False|\n",
      "new state: 298 | reward: -10| done: False|\n",
      "new state: 198 | reward: -1| done: False|\n",
      "new state: 198 | reward: -1| done: False|\n",
      "new state: 198 | reward: -1| done: False|\n",
      "new state: 98 | reward: -1| done: False|\n",
      "new state: 98 | reward: -10| done: False|\n",
      "new state: 98 | reward: -1| done: False|\n",
      "new state: 98 | reward: -1| done: False|\n",
      "new state: 78 | reward: -1| done: False|\n",
      "new state: 98 | reward: -1| done: False|\n",
      "new state: 98 | reward: -1| done: False|\n",
      "new state: 198 | reward: -1| done: False|\n",
      "new state: 198 | reward: -10| done: False|\n",
      "new state: 198 | reward: -10| done: False|\n",
      "new state: 198 | reward: -1| done: False|\n",
      "new state: 198 | reward: -10| done: False|\n",
      "new state: 98 | reward: -1| done: False|\n",
      "new state: 98 | reward: -10| done: False|\n",
      "new state: 78 | reward: -1| done: False|\n",
      "new state: 78 | reward: -10| done: False|\n",
      "new state: 78 | reward: -10| done: False|\n",
      "new state: 58 | reward: -1| done: False|\n",
      "new state: 58 | reward: -1| done: False|\n",
      "new state: 78 | reward: -1| done: False|\n",
      "new state: 78 | reward: -10| done: False|\n",
      "new state: 78 | reward: -10| done: False|\n",
      "new state: 78 | reward: -10| done: False|\n",
      "new state: 78 | reward: -10| done: False|\n",
      "new state: 78 | reward: -1| done: False|\n",
      "new state: 78 | reward: -1| done: False|\n",
      "new state: 178 | reward: -1| done: False|\n",
      "new state: 78 | reward: -1| done: False|\n",
      "new state: 58 | reward: -1| done: False|\n",
      "new state: 58 | reward: -10| done: False|\n",
      "new state: 58 | reward: -10| done: False|\n",
      "new state: 58 | reward: -10| done: False|\n",
      "new state: 58 | reward: -1| done: False|\n",
      "new state: 78 | reward: -1| done: False|\n",
      "new state: 58 | reward: -1| done: False|\n",
      "new state: 78 | reward: -1| done: False|\n",
      "new state: 78 | reward: -10| done: False|\n",
      "new state: 178 | reward: -1| done: False|\n",
      "new state: 178 | reward: -10| done: False|\n",
      "new state: 178 | reward: -10| done: False|\n",
      "new state: 78 | reward: -1| done: False|\n",
      "new state: 178 | reward: -1| done: False|\n",
      "new state: 278 | reward: -1| done: False|\n",
      "new state: 258 | reward: -1| done: False|\n",
      "new state: 158 | reward: -1| done: False|\n",
      "new state: 258 | reward: -1| done: False|\n",
      "new state: 358 | reward: -1| done: False|\n",
      "new state: 458 | reward: -1| done: False|\n",
      "new state: 458 | reward: -10| done: False|\n",
      "new state: 438 | reward: -1| done: False|\n",
      "new state: 438 | reward: -1| done: False|\n",
      "new state: 438 | reward: -10| done: False|\n",
      "new state: 438 | reward: -10| done: False|\n",
      "new state: 338 | reward: -1| done: False|\n",
      "new state: 338 | reward: -1| done: False|\n",
      "new state: 338 | reward: -10| done: False|\n",
      "new state: 338 | reward: -1| done: False|\n",
      "new state: 338 | reward: -1| done: False|\n",
      "new state: 238 | reward: -1| done: False|\n",
      "new state: 138 | reward: -1| done: False|\n",
      "new state: 118 | reward: -1| done: False|\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1\n",
    "decay_factor = 0.98\n",
    "train_episodes = 2000\n",
    "replay_size = 64\n",
    "\n",
    "moves_limit = 200\n",
    "memory = Memory(2000)\n",
    "memory.kickstart(environment,6,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = environment.reset()\n",
    "\n",
    "for ep in range(episodes):\n",
    "    for i in range(moves_limit):\n",
    "        probability = epsilon_greedy_policy(epsilon,state, 6, Q)\n",
    "        action = np.random.choice(np.arange(6),p=probability)\n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "        memory.add((state, action, reward, new_state, done))\n",
    "        \n",
    "    replay = memory.sample(replay_size)\n",
    "    for experience in replay:\n",
    "        state, action, reward, new_state, done = experience\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 9, 2, 4, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(78, 3, -1, 58, False), (438, 0, -1, 438, False), (98, 2, -1, 98, False), (98, 3, -1, 78, False), (474, 5, -10, 474, False)]\n"
     ]
    }
   ],
   "source": [
    "a = memory.sample(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
